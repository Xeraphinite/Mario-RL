#### Super Mario 基本介绍

超级马力欧系列（日语：スーパーマリオ，英语：Super Mario）是任天堂开发，以旗下吉祥物——马力欧为主角的动作平台游戏系列，为《马力欧系列》中的核心作品，累计销量达 3 亿 1000 万份，为多个马力欧相关系列里最高，也帮助整个马力欧系列成为世界上销量最高的电子游戏系列。任天堂各主要家用机和掌上机上市后，平台上至少都有一部超级马力欧的新作。

在超级马力欧游戏中，玩家通常控制马力欧，在虚构的蘑菇王国中冒险。同行的一般还有其弟路易吉，偶尔或是其它马力欧角色。玩家操控角色在每关的平台上奔跑、跳跃，并从上方踩敌人头部。游戏剧情不复杂，典型的是马力欧营救被反派酷霸王绑架的桃花公主。系列首作是 1985 年红白机游戏《超级马力欧兄弟》，它确立了系列的基本游戏概念和元素。游戏中有各种道具，可以提升马力欧的能力，比如拥有投射火球的能力，或是变大缩小。

本次让强化学习 Agent 玩的游戏是最经典的，在红白机上的《**超级马力欧兄弟**》.

#### 强化学习基本概念

##### 基本介绍

强化学习(*Reinforcement Learning*, **RL**)讨论的问题是智能体(*agent*)怎么在一个复杂、不确定的环境(*environment*)里面去最大化它能获得的奖励. 示意图由两部分组成：智能体和环境. 在强化学习过程中, 智能体跟环境一直进行交互, 智能体在环境里面获取到状态(*state*), 并利用这个状态输出一个动作(*action*), 这个动作也被称为决策(*decision*). 然后这个决策会在环境中执行, 环境会根据智能体所采取的决策, 输出下一个状态以及当前的这个决策得到的奖励. 智能体的目的就是为了尽可能多地从环境中获取奖励. 

![img](E:\桌面\RL.assets\1.1.png)

近年来, 由于机器算力的提升和神经网络、深度学习的发展, 强化学习可以和深度学习结合到一起, 就形成了深度强化学习(*deep reinforcement learning*), 因此, 深度强化学习 = 深度学习 + 强化学习. 通过深度强化学习, 智能体与环境交互的过程就可以改进为一个端到端(*end-to-end*)的训练过程, 此时, 我们无需设计特征, 直接输入状态, 并定义神经网络的基本架构, 就可以输出 Action. 同时, 我们可以使用一个神经网络来拟合价值函数(*value function*), Q 函数或者策略网络, 省去特征工程(*feature engineering*)的过程.

##### 序列决策过程, Sequential Decision Making

强化学习研究的是智能体与环境交互的问题, 智能体会对环境输出动作, 环境取得动作之后进行下一步, 并把下一步的观测(observation)以及这个动作带来的奖励(reward)返还给 智能体. 智能体的目的就是从这些观测中学习到能够最大化奖励的策略. 

奖励是由环境给智能体的一个标量反馈信号, 这种信号可以显示智能体在某一步采取某个策略的表现如何. 强化学习的目的就是最大化智能体可以获得的奖励, 因此, 智能体在环境中存在的目的就是最大化期望的累计奖励（*expected cumulative reward*）. 

绝大多数的强化学习环境中, 智能体的奖励是被延迟了的了——我们选取的某一个动作, 可能需要很久之后才能知道这一步到底产生了什么影响. 强化学习中一个重要的课程就是近期奖励和远期奖励的权衡（*trade-off*）, 研究怎样让智能体获得更多的远期奖励.

在与环境交互的过程中, 智能体会获得很多观测, 针对每一个观测, 智能体会采取一个动作, 并得到一个奖励, 因此历史可以当做一个观测、动作、奖励的序列：
$$
H_t = o_1,a_1,r_1,\dots,o_t,a_t,r_t
$$
因此, 整个游戏的状态可以当做是关于这个历史的函数：
$$
s_t= f(H_t)
$$
需要注意的是状态和观测的关系：状态 $s$ 是对世界的完整描述, 不会隐藏世界的信息. 观测 $o$ 是对状态的部分描述, 可能会遗漏一些信息. 很多时候, 状态和观测是并不对等的. 当智能体只能够看到一部分的信息的时候, 我们就称这个环境是部分可观测(*partially observed*)的, 此时, 强化学习被建模为部分可观测的马尔科夫决策过程(*partially observable Markov decision process, POMDP*). 这个过程可以用一个七元组描述：
$$
(S,A,T,R,\Omega,O,\gamma)
$$
其中 $S$ 表示状态空间, 为隐变量, $A$ 为动作空间, $T(s'|s,a)$ 为状态转移概率, $R$ 为奖励函数, $\Omega(o|s,a)$ 为观测概率, O 为观测空间, $\gamma$ 为折扣系数. 

##### 动作空间, Action Space

不同的环境允许不同种类的动作. 在给定的环境中, 有效动作的集合经常被称为动作空间(*action space*). 像 Atari 和 Go 这样的环境有离散动作空间(*discrete action spaces*), 在这个动作空间里, agent 的动作数量是有限的. 在其他环境, 比如在物理世界中控制一个 agent, 在这个环境中就有连续动作空间(*continuous action spaces*) . 在连续空间中, 动作是实值的向量. 

##### 强化学习智能体的组成部分和类型

对于一个强化学习 agent, 它可能有一个或多个如下的组成成分：

- 策略函数(*policy function*), agent 会用这个函数来选取下一步的动作. 
- 价值函数(*value function*), 我们用价值函数来对当前状态进行估价, 它就是说你进入现在这个状态, 可以对你后面的收益带来多大的影响. 当这个价值函数大的时候, 说明你进入这个状态越有利. 
- 模型(*model*), 模型表示了 agent 对这个环境的状态进行了理解, 它决定了这个世界是如何进行的. 

其中, Policy 是 agent 的行为模型, 它决定了这个 agent 的行为, 它其实是一个函数, 把输入的状态变成行为. 强化学习一般使用随机性策略(*stochastic policy*), 其实也就是 $\pi$ 函数： $\pi(a | s)=P\left[A_{t}=a | S_{t}=s\right]$ . 当你输入一个状态 $s$ 的时候, 输出是一个概率. 这个概率是智能体所有动作的概率, 对这个概率分布进行采样, 就能得到智能体即将采取的动作.

价值函数是对未来奖励的预测, 我们用它来评估当前状态的好坏. 价值函数由一个期望定义：
$$
v_{\pi}(s) \doteq \mathbb{E}{\pi}\left[G{t} \mid S_{t}=s\right]=\mathbb{E}{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right], \text {for all } s \in \mathcal{S}
$$
其中, $\gamma$ 是一个折扣因子(*discount factor*). $\gamma$ 越大 agent 往前考虑的步数越多，但训练难度也越高；$\gamma$ 越小agent越注重眼前利益，训练难度也越小。我们都希望 agent 能“深谋远虑”，但过高的折扣因子容易导致算法收敛困难，因此作为强化学习的超参数，调整 $\gamma$ 的值是非常重要的. 总的来说，因此折扣因子的选择应当在算法能够收敛的前提下尽可能大. 

期望 $\mathbb E_\pi$ 的下标是 $\pi$ 函数, 它的值能够反映出我们在使用 $\pi$ 函数作为策略的时候, 到底能够得到多少奖励.

另一种价值函数被称为 $Q$ 函数, 定义如下：
$$
Q_{\pi}(s, a) \doteq \mathbb{E}{\pi}\left[G{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right]
$$
$Q$ 函数包含了两个变量：状态和动作. 从 $Q$ 函数的定义也可以知道, 它就是我们在强化学习算法中需要学习的函数——因为当我们得到 $Q$ 函数之后, 进入某个状态需要采取的最优动作可以通过 $Q$ 函数得到.

第三个组成部分是模型, 它决定了下一步的状态. 下一步的状态取决于当前的状态以及当前采取的动作. 它由状态转移概率和奖励函数两个部分组成，状态转移概率也就是：
$$
p^a_{ss'}=p(s_{t+1}=s'|s_t=s, a_t=a)
$$
奖励函数表示我们在当前状态采取了某个动作，可以得到多少奖励：
$$
\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]
$$
当有了策略、价值函数、模型三个部分之后，就形成了一个马尔科夫决策过程，这也是强化学习建模中常用的思路. 

##### 强化学习 Agent 的类型

根据智能体学习事物的不同, 可以将强化学习智能体分为基于价值的智能体(*value-based agent*) 以及基于策略的智能体(*policy-based agent*). 基于价值的智能体通过显式学习价值函数, 隐式地学习策略. 而基于策略的智能体直接学习策略, 给它一个状态, 它就会输出对应动作的概率. 将两种学习方式结合就有了演员-评论员智能体(*actor-critic agent*), 这类智能体通过学习策略函数、价值函数以及它们之间的交互得到最佳的动作.

我们也可以通过智能体是否学习环境模型将智能体分为有模型 (*model-based*) 以及免模型 (*model-free*) 智能体. 由于智能体在实际应用中较难估计状态转移函数和奖励函数, 甚至环境中的状态都是未知的, 因此需要采用免模型强化学习, 无需对真实环境建模. 免模型强化学习需要大量的采样来估计状态、动作以及价值函数, 从而优化策略. 大部分深度强化学习方法都采用了免模型强化学习.

##### 探索与利用

在强化学习中，探索(*exploration*)和利用(*exploitation*)是两个非常核心的问题. 探索是让我们尝试去探索环境，通过尝试不同的动作得到最佳的策略; 利用则是我们不去尝试新的动作，直接用当前已知的，能够带来最大奖励的策略. 对于探索和利用的关系，有以下值得注意的几个点：

1. 充分的探索才能带来有效的利用，从而使强化学习走在正确的道路上。对于那些难度特别高的任务，改进探索策略是性价比最高的手段;
2. 充分的利用才能探索到更好的状态，agent 往往需要掌握基本技能，才能解锁更高级的技能。就好像小孩先要学会站起来，才能学会走，然后才能学会跑。这种从易到难、循序渐进的思想在RL中也很受用;
3. 过量的探索阻碍及时的利用。如果随机探索噪声强度过高，已经学到的知识会被噪声淹没，而无法指导 agent解锁更好的状态，导致强化学习模型的性能停滞不前；
4. 机械的利用误导探索的方向。如果刚刚学到一点知识就无条件利用，agent 有可能被带偏，从而陷入局部最优，在错误道路上越走越远，在训练早期就扼杀了最好的可能性.

总而言之，强化学习是一个探索和利用的平衡游戏，前者使 agent 充分遍历环境中的各种可能性，从而有机会找到最优解；后者利用学到的经验指导 agent 做出更合理的选择。

##### 强化学习基本流程、与监督学习的差别

完整的强化学习过程需要以下几个步骤：

1. 搭建强化学习环境
2. 训练模型



#### Deep-Q Network 原理

##### DQN

传统的强化学习算法会使用表格的形式存储状态值函数 $V⁡(s)$ 或状态动作值函数 Q$⁡(s,a)$，但是这样的方法存在很大的局限性。例如：现实中的强化学习任务所面临的状态空间往往是连续的，存在无穷多个状态，在这种情况下，就不能再使用表格对值函数进行存储。值函数近似利用函数直接拟合状态值函数或状态动作值函数，减少了对存储空间的要求，有效地解决了这个问题。

为了在连续的状态空间中计算价值函数 Qπ⁡(s,a)，我们可以用一个函数 Qϕ⁡(s,a) 来表示近似计算，称为价值函数近似(Value Function Approximation)。 
$$
Q_{\phi}(\boldsymbol{s}, \boldsymbol{a}) \approx Q^{\pi}(s, a)
$$
其中, $\boldsymbol{s}, \boldsymbol{a}$ 分别是状态 $s$ 和动作 $a$ 的向量表示，函数 $Q_\phi(s,a)$ 通常是一个参数为 $\phi$ 的函数，比如神经网络，输出为一个实数，称为 Q 网络(Q-network)。

深度Q网络（Deep Q-Network，DQN）算法的核心是维护 Q 函数并使用其进行决策。$Q_\pi(s, a)$ 为在该策略 $\pi$ 下的动作价值函数，每次到达一个状态 $s_t$ 之后，遍历整个动作空间，使用让 $Q_\pi⁡(s,a)$ 最大的动作作为策略：
$$
a_{t}=\underset{a}{\arg \max } ~Q^{\pi}\left(s_{t}, a\right)
$$
DQN 采用贝尔曼方程来迭代更新 $Q^{\pi}\left(s_{t}, a_{t}\right)$ ： 
$$
Q^{\pi}\left(s_{t}, a_{t}\right) \leftarrow Q^{\pi}\left(s_{t}, a_{t}\right)+\alpha\left(r_{t}+\gamma \max *{a} Q^{\pi}\left(s*{t+1}, a\right)-Q^{\pi}\left(s_{t}, a_{t}\right)\right)
$$
通常在简单任务上，使用全连接神经网络（*fully connected neural network*）来拟合 $Q_\pi$，但是在较为复杂的任务上（如玩 Atari Game），会使用卷积神经网络来拟合从图像到价值函数的映射。由于 DQN 的这种表达形式只能处理有限个动作值，因此其通常用于处理离散动作空间的任务。

##### Epsilon Greedy



##### Tricks - Double DQN

在 DQN 的实现中，最经常使用到的 Tricks 就是 Double DQN 了.



#### 实现强化学习

##### 任务目标

通过 Reinforcement Learning 训练 Agent 玩超级玛丽游戏，并取得尽可能高的 Reward.

##### 问题定义、动作空间、状态空间

为了简化问题，在 Super Mario Bros. 的环境中，Agent 的动作空间如下
$$
\set{walk, jump}
$$
其中 $walk$ 代表让 Mario 向右走，而 $jump$ 代表让 Mario 向右跳.

Agent 的状态空间则是一个 shape 为 $(240, 256, 3)$ 的 Tensor，

##### 稀疏回报、设计奖励

在大多数时候，强化学习的 Agent 都无法获得奖励，在无法得到奖励的情况下，训练智能体是非常困难的. 因此，我们需要更加细化的奖励数据来引导 Agent 走向正确的道路，这种方式也叫做设计奖励(*reward shaping*, 或 *credit assignment*). 这种设计奖励的方式是需要领域知识(*domain knowledge*) 的.

##### 奖励函数, Reward Function



##### 算法选择



#### 代码实现

##### 实验环境

软件: Windows 11, Python 3.9.9 with torch-gpu

CPU: Intel(R) Core(TM) i5-9300H

GPU: Nvidia GTX 1050

由于经济能力和设备的限制，同时又因为强化学习对设备算力极其严苛的要求，本次课程设计可能不会有太好的结果，未来如果能够有机会使用到更好的设备重新 train 的话效果可能会好很多. 同时，由于机器学习的可解释性问题，大部分的参数几乎都选择的是经验参数，好的参数可以将训练过程缩减、并让训练效果大大提升，当然，也需要长期的积累和持续的努力.

##### Mario 环境准备

考虑到 OpenAI 的 gym 已经有了非常多

```Python
# 0. 导入环境
env = gym_super_mario_bros.make("SuperMarioBros-1-1-v0") # 导入环境
env = JoypadSpace(env, [["right"], ["right", "A"]]) # 限制 Action 在向右走和向右跳之间
env.reset() # 在使用 environment 之前, 需要 reset， 以初始化环境
```

##### Agent - Mario



```Python

```



##### Network define

核心的网络由三层 (Conv2d + ReLU) 和 (Dense + ReLU) 构成，

```Python
from torch import nn

nn.Sequential(
    nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),
    nn.ReLU(),
    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
    nn.ReLU(),
    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear(3136, 512),
    nn.ReLU(),
    nn.Linear(512, output_dim),
)
```

##### Training

在强化学习中，最经常使用的库就是 OpenAI 的 gym 了. 

```Python

```

##### Predicting - Play The Game

在强化学习中，predict 的过程也就是利用 model 玩游戏的过程，代码和 Agent 在训练的时候进行交互的核心代码几乎是一致的.



```Python
state = env.reset() # 开始游戏
while True: 
    action, _ = model.predict(state) # 使用上述模型在当前的 state 下输出一个 action
    state, reward, done, info = env.step(action) # 和 Enviornment 交互, 得到下一个 action
    env.render() # 渲染当前帧
```

#### 成果展示

训练了 10000 个 Episode 之后的视频：

训练了 50000 个 Episode 之后的视频：

#### 待改进的部分

1. reward 的设置基本上是没有

#### 总结

强化学习需要达到的泛化能力是非常高的，同时，对于数据的需求更是比监督学习高得多（甚至需要一个 simulator 不断重复地生产数据，由此可知强化学习对于数据的苛刻程度），当然，强化学习也可以做到普通的监督学习做不到的事情。

强化学习

#### 参考资料

1. Easy RL - 强化学习教程（https://datawhalechina.github.io/easy-rl/#/）
2. Shusen Wang - 深度强化学习（https://github.com/wangshusen/DeepLearning）
3. 深度强化学习落地指南（https://www.zhihu.com/column/c_1186982555915599872）
4. 